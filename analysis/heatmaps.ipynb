{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4d805a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    2023: {\"cluster_file\": \"clusters_2023_sustantivas_5_v3\", \"tree_file\": \"../data/estructura/2023_12_09.csv\"},\n",
    "    2025: {\"cluster_file\": \"clusters_2025_sustantivas_5_v3\", \"tree_file\": \"../data/estructura/2025_07_08.csv\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d0b3c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2025\n",
    "central_administration_only = True\n",
    "objective_threshold = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ea3266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener ocurrencias de un unico cluster\n",
    "from chainsaw.heatmaps.llm_extraction import LLMExtraction\n",
    "from chainsaw.heatmaps.constants import DimensionName\n",
    "\n",
    "cluster_id = 0\n",
    "LLMExtraction(data[year][\"cluster_file\"]).execute(\n",
    "    cluster_id,\n",
    "    reuse=[\n",
    "        DimensionName.OBJECTIVE,\n",
    "        DimensionName.TARGET\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2477f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener ocurrencias en cada dimension para utilizarlas en la creacion de heatmaps\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from chainsaw.heatmaps.llm_extraction import LLMExtraction\n",
    "from chainsaw.heatmaps.constants import DimensionName\n",
    "\n",
    "clusters_to_omit = [0]\n",
    "\n",
    "with open(f\"clusters/{data[year]['cluster_file']}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    cluster_file_content = json.load(f)\n",
    "    clusters_by_size = sorted(\n",
    "        Counter(\n",
    "            each[\"cluster\"]\n",
    "            for each in cluster_file_content[\"clusters_data\"]\n",
    "            if each[\"cluster\"] not in clusters_to_omit).items(),\n",
    "        key=lambda c: c[1])\n",
    "\n",
    "for cluster, size in tqdm(clusters_by_size, total=len(clusters_by_size), desc=\"Cluster\", leave=False):\n",
    "    LLMExtraction(data[year][\"cluster_file\"]).execute(\n",
    "        cluster,\n",
    "        reuse=[\n",
    "            # DimensionName.OBJECTIVE,\n",
    "            # DimensionName.TARGET,\n",
    "            # DimensionName.ENVIRONMENT,\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f640f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distintos pesos para cada dimensión, generando múltiples escenarios posibles para los outputs json\n",
    "WEIGHTS_TO_EXECUTE = [\n",
    "  {\"objetivos\": 0.6, \"distancia\": 0.2, \"destinatarios\": 0.1, \"ambitos\": 0.1},\n",
    "  {\"objetivos\": 0.5, \"distancia\": 0.3, \"destinatarios\": 0.1, \"ambitos\": 0.1},\n",
    "  {\"objetivos\": 0.4, \"distancia\": 0.4, \"destinatarios\": 0.1, \"ambitos\": 0.1},\n",
    "  {\"objetivos\": 0.5, \"distancia\": 0.4, \"destinatarios\": 0.1, \"ambitos\": 0},\n",
    "  {\"objetivos\": 0.5, \"distancia\": 0.45, \"destinatarios\": 0.05, \"ambitos\": 0},\n",
    "  {\"objetivos\": 0.6, \"distancia\": 0.3, \"destinatarios\": 0.1, \"ambitos\": 0},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8d28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar archivo json final, con todos los heatmaps del año indicado\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from chainsaw.heatmaps.plot import (\n",
    "    partial_matrixes,\n",
    "    _final_matrix,\n",
    "    _apply_dimension_weights,\n",
    ")\n",
    "\n",
    "\n",
    "def statistics_for_cluster(units_order, matrixes, weights, unit_similarity_threshold):\n",
    "    statistics = {}\n",
    "    units_amount = len(units_order)\n",
    "    units_with_position = {unit_data[\"idx\"]: unit_data[\"unit\"] for unit_uuid, unit_data in units_order.items()}\n",
    "\n",
    "    units_by_jurisdiction = defaultdict(list)\n",
    "    for unit_idx, unit in units_with_position.items():\n",
    "        units_by_jurisdiction[unit[\"jurisdiction\"]].append(unit)\n",
    "    cluster_size = units_amount * units_amount\n",
    "\n",
    "    statistics[\"cant_unidades\"] = units_amount\n",
    "    statistics[\"cant_diadas\"] = cluster_size\n",
    "    statistics[\"cant_jurisdicciones\"] = len(units_by_jurisdiction.keys())\n",
    "    statistics[\"cant_unidades_por_jurisdiccion\"] = {j: len(units) for j, units in units_by_jurisdiction.items()}\n",
    "    statistics[\"superposicion\"] = {}\n",
    "\n",
    "    for strict in (True, False):\n",
    "        similarity_statistics = {}\n",
    "        matrix = _final_matrix(units_order, matrixes, weights, strict)\n",
    "        total = sum(sum(row) for row in matrix)\n",
    "        average = total / cluster_size\n",
    "\n",
    "        highest_similarities = []\n",
    "        highest_units = set()\n",
    "        for i in range(units_amount):\n",
    "            for j in range(i+1, units_amount):\n",
    "                similarity = matrix[i][j]\n",
    "                if (similarity >= unit_similarity_threshold)\\\n",
    "                    and (matrixes[\"objetivos\"][i][j] > (objective_threshold * weights[\"objetivos\"]))\\\n",
    "                    and (matrixes[\"distancia\"][i][j] > 0):\n",
    "                    highest_similarities.append({\n",
    "                        \"fila\": i,\n",
    "                        \"columna\": j,\n",
    "                        \"unidad_fila\": units_with_position[i][\"name\"],\n",
    "                        \"unidad_columna\": units_with_position[j][\"name\"],\n",
    "                        \"uuid_fila\": units_with_position[i][\"uuid\"],\n",
    "                        \"uuid_columna\": units_with_position[j][\"uuid\"],\n",
    "                        \"jurisdiccion_fila\": units_with_position[i][\"jurisdiction\"],\n",
    "                        \"jurisdiccion_columna\": units_with_position[j][\"jurisdiction\"],\n",
    "                        \"path_fila\": units_with_position[i][\"path\"],\n",
    "                        \"path_columna\": units_with_position[j][\"path\"],\n",
    "                        \"similitud_coseno\": similarity,\n",
    "                    })\n",
    "                    highest_units.add(units_with_position[i][\"uuid\"])\n",
    "                    highest_units.add(units_with_position[j][\"uuid\"])\n",
    "\n",
    "        similarity_statistics[\"suma_superposicion\"] = total\n",
    "        similarity_statistics[\"promedio_superposicion\"] = average\n",
    "        similarity_statistics[\"cant_diadas_superposicion_alta\"] = len(highest_similarities)\n",
    "        similarity_statistics[\"cant_unidades_superposicion_alta\"] = len(highest_units)\n",
    "        similarity_statistics[\"porcentaje_unidades_superpuestas\"] = len(highest_units) / units_amount * 100\n",
    "        similarity_statistics[\"diadas_superposicion_alta\"] = highest_similarities\n",
    "        key = \"estricto\" if strict else \"no_estricto\"\n",
    "        statistics[\"superposicion\"][key] = similarity_statistics\n",
    "    return statistics\n",
    "\n",
    "\n",
    "def global_statistics(\n",
    "    global_units_amount,\n",
    "    global_clustered_units_amount,\n",
    "    global_pairs_amount,\n",
    "    global_sum_all_similarities,\n",
    "    global_highest_similarity_units_amount,\n",
    "    unit_similarity_threshold,\n",
    "    clusters_to_omit,\n",
    "    weights,\n",
    "):\n",
    "    statistics = {}\n",
    "    similarity_statistics = {}\n",
    "    for key in (\"estricto\", \"no_estricto\"):\n",
    "        similarity_statistics[key] = {\n",
    "            \"cant_unidades_superpuestas\": global_highest_similarity_units_amount[key],\n",
    "            \"promedio_global_superposicion\": global_sum_all_similarities[key] / global_pairs_amount,\n",
    "            \"porcentaje_superposicion_unidades_clusterizadas\": global_highest_similarity_units_amount[key] / global_clustered_units_amount * 100,\n",
    "            \"porcentaje_superposicion_unidades_totales\": global_highest_similarity_units_amount[key] / global_units_amount * 100,\n",
    "        }\n",
    "    statistics[\"cant_global_unidades\"] = global_units_amount\n",
    "    statistics[\"cant_unidades_clusterizadas\"] = global_clustered_units_amount\n",
    "    statistics[\"cant_unidades_restantes\"] = statistics[\"cant_global_unidades\"] - global_clustered_units_amount\n",
    "    statistics[\"umbral_similitud\"] = unit_similarity_threshold\n",
    "    statistics[\"superposicion\"] = similarity_statistics\n",
    "    statistics[\"clusters_omitidos\"] = clusters_to_omit\n",
    "    statistics[\"ponderadores\"] = weights\n",
    "    return statistics\n",
    "\n",
    "\n",
    "for cosine_similarity_threshold in [0.70]:\n",
    "    for unit_similarity_threshold in [0.6]:\n",
    "        for year, limit, clusters_to_omit in [(2023, 1, [-1]), (2025, 13, [-1])]:\n",
    "            for weights in WEIGHTS_TO_EXECUTE:\n",
    "                with open(f\"clusters/{data[year][\"cluster_file\"]}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                    cluster_file_content = json.load(f)\n",
    "\n",
    "                cluster_file_content[\"heatmaps\"] = {}\n",
    "                global_clustered_units_amount = 0\n",
    "                global_pairs_amount = 0\n",
    "                global_omited_units_amount = 0\n",
    "                global_sum_all_similarities = defaultdict(float)\n",
    "                global_highest_similarity_units_amount = defaultdict(int)\n",
    "\n",
    "                all_matrixes = {}\n",
    "                for cluster_id in range(-1, limit):\n",
    "                    matrixes, units_order = partial_matrixes(\n",
    "                        data[year][\"cluster_file\"],\n",
    "                        cluster_id,\n",
    "                        data[year][\"tree_file\"],\n",
    "                        central_administration_only,\n",
    "                        cosine_similarity_threshold,\n",
    "                        list(weights.keys()),\n",
    "                    )\n",
    "                    all_matrixes[cluster_id] = {\"matrixes\": matrixes, \"units_order\": units_order}\n",
    "\n",
    "                all_matrixes = _apply_dimension_weights(all_matrixes, weights)\n",
    "                for cluster_id in range(0, limit):\n",
    "                    matrixes = all_matrixes[cluster_id][\"matrixes\"]\n",
    "                    units_order = all_matrixes[cluster_id][\"units_order\"]\n",
    "                    cluster_data = {}\n",
    "                    cluster_data[\"orden\"] = [{\"uuid\": unit[\"uuid\"], \"nombre\": unit[\"name\"], \"jurisdiccion\": unit[\"jurisdiction\"]}\n",
    "                                            for e in sorted(units_order.values(), key=lambda each: each[\"idx\"])\n",
    "                                            if (unit:=e[\"unit\"])]\n",
    "                    cluster_data.update(matrixes)\n",
    "                    cluster_data[\"estadisticas\"] = statistics_for_cluster(units_order, matrixes, weights, unit_similarity_threshold)\n",
    "                    cluster_file_content[\"heatmaps\"][str(cluster_id)] = cluster_data\n",
    "\n",
    "                    # Acumulamos estadisticas globales\n",
    "                    if cluster_id in clusters_to_omit:\n",
    "                        global_omited_units_amount = global_omited_units_amount + cluster_data[\"estadisticas\"][\"cant_unidades\"]\n",
    "                    else:\n",
    "                        global_clustered_units_amount = global_clustered_units_amount + cluster_data[\"estadisticas\"][\"cant_unidades\"]\n",
    "                        global_pairs_amount = global_pairs_amount + cluster_data[\"estadisticas\"][\"cant_diadas\"]\n",
    "                        for key in (\"estricto\", \"no_estricto\"):\n",
    "                            global_sum_all_similarities[key] = global_sum_all_similarities[key] + cluster_data[\"estadisticas\"][\"superposicion\"][key][\"suma_superposicion\"]\n",
    "                            global_highest_similarity_units_amount[key] = global_highest_similarity_units_amount[key] + cluster_data[\"estadisticas\"][\"superposicion\"][key][\"cant_unidades_superposicion_alta\"]\n",
    "\n",
    "                # Guardamos estadísticas globales\n",
    "                cluster_file_content[\"estadisticas\"] = global_statistics(\n",
    "                    len(cluster_file_content[\"objectives\"].keys()) - global_omited_units_amount,\n",
    "                    global_clustered_units_amount,\n",
    "                    global_pairs_amount,\n",
    "                    global_sum_all_similarities,\n",
    "                    global_highest_similarity_units_amount,\n",
    "                    unit_similarity_threshold,\n",
    "                    clusters_to_omit,\n",
    "                    weights,\n",
    "                )\n",
    "\n",
    "                sufix = f\"obj{int(weights['objetivos']*10)}_dist{int(weights['distancia']*10)}_dest{int(weights['destinatarios']*10)}_amb{int(weights['ambitos']*10)}_threshold{int(unit_similarity_threshold*10)}_cos_sim{int(cosine_similarity_threshold*100)}\"\n",
    "                with open(f\"finals/{data[year][\"cluster_file\"]}_{sufix}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(cluster_file_content, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chainsaw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
